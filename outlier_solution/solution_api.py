# solution.py
import os
import glob
import pandas as pd

# ì„ íƒ ì‚¬í•­: chardet ìˆìœ¼ë©´ ì¸ì½”ë”© ì¶”ì •ì— ì‚¬ìš©, ì—†ìœ¼ë©´ ê·¸ëƒ¥ ë„˜ì–´ê°
try:
    import chardet
except ImportError:
    chardet = None


# âœ… ì›ë³¸ CSVë“¤ì´ ìˆëŠ” í´ë”
BASE_PATH = "/content/drive/MyDrive/middle_project/ì¤‘ê°„í‰ê°€_ë¶„ì„íŒŒì¼"

# âœ… í´ë¦° CSVë“¤ì„ ì €ì¥í•  í´ë” (ì›í•˜ë©´ ë‹¤ë¥¸ ê²½ë¡œë¡œ ë³€ê²½ ê°€ëŠ¥)
OUTPUT_DIR = "/content/drive/MyDrive/middle_project/ì¤‘ê°„í‰ê°€_ë¶„ì„íŒŒì¼_clean"

# âœ… ì‹œë„í•´ë³¼ ì¸ì½”ë”© í›„ë³´ë“¤
TRY_ENCODINGS = ['utf-8-sig', 'utf-8', 'cp949', 'euc-kr']



def guess_encoding(path, n_bytes=200000):
    """chardetê°€ ìˆìœ¼ë©´ íŒŒì¼ ì¼ë¶€ë¥¼ ë³´ê³  ì¸ì½”ë”© ì¶”ì •, ì—†ìœ¼ë©´ None."""
    if chardet is None:
        return None
    with open(path, "rb") as f:
        raw = f.read(n_bytes)
    result = chardet.detect(raw)
    return result.get("encoding")


def read_csv_robust(path):
    """
    ì—¬ëŸ¬ ì¸ì½”ë”©ìœ¼ë¡œ ì½ì–´ë³´ê³ ,
    ???, ï¿½ ê°™ì€ ê¹¨ì§„ ë¬¸ì ë¹„ìœ¨ì´ ê°€ì¥ ë‚®ì€ ê²°ê³¼ë¥¼ ì„ íƒ.
    ì „ë¶€ ì• ë§¤í•˜ë©´ ê·¸ë‚˜ë§ˆ ë‚˜ì€ ê±¸ ì“°ê³ , ìµœì•…ì´ë©´ utf-8 + replaceë¡œ ê°•ì œ ë¡œë“œ.
    """
    tried = []
    enc_guess = guess_encoding(path)

    if enc_guess:
        try_order = [enc_guess] + [e for e in TRY_ENCODINGS if e != enc_guess]
    else:
        try_order = TRY_ENCODINGS[:]

    best_df = None
    best_enc = None
    best_bad = 1.0
    last_err = None

    for enc in try_order:
        if enc in tried:
            continue
        tried.append(enc)

        try:
            df = pd.read_csv(path, encoding=enc)
        except Exception as e:
            last_err = e
            continue

        # ì»¬ëŸ¼ëª… + ì²« í–‰ì„ ìƒ˜í”Œë¡œ ê°€ì ¸ì™€ì„œ ê¹¨ì§ ì •ë„ í™•ì¸
        text_sample = " ".join(map(str, list(df.columns)[:30]))
        if len(df) > 0:
            text_sample += " " + " ".join(map(str, df.iloc[0].astype(str).tolist()))

        # ???, ï¿½ ë¹„ìœ¨ ê³„ì‚°
        bad = (text_sample.count('?') + text_sample.count('ï¿½')) / max(len(text_sample), 1)

        # ê±°ì˜ ì•ˆ ê¹¨ì¡Œìœ¼ë©´ ë°”ë¡œ í™•ì •
        if bad < 0.01:
            return df, enc

        # ê·¸ ì™¸ì—ëŠ” "í˜„ì¬ê¹Œì§€ ì¤‘ì— ì œì¼ ëœ ê¹¨ì§„ í›„ë³´"ë¡œ ì €ì¥
        if bad < best_bad:
            best_bad = bad
            best_df = df
            best_enc = enc

    # ê·¸ë˜ë„ ì½íŒ ê²Œ í•˜ë‚˜ë¼ë„ ìˆìœ¼ë©´ ê·¸ê²ƒ ì‚¬ìš©
    if best_df is not None:
        if best_bad > 0.1:
            print(f"[ê²½ê³ ] {os.path.basename(path)}: ì¸ì½”ë”©ì´ ì™„ë²½í•˜ì§„ ì•Šì„ ìˆ˜ ìˆìŒ (ê¹¨ì§ ë¹„ìœ¨ {best_bad:.2f}), ì‚¬ìš© enc={best_enc}")
        return best_df, best_enc

    # ì „ë¶€ ì‹¤íŒ¨í•˜ë©´ utf-8 + replaceë¡œ ê°•ì œ ë¡œë“œ (ê¹¨ì§„ ê¸€ìëŠ” ï¿½ë¡œ ë“¤ì–´ê°ˆ ìˆ˜ ìˆìŒ)
    try:
        df = pd.read_csv(path, encoding="utf-8", encoding_errors="replace")
    except Exception as e:
        raise RuntimeError(f"{os.path.basename(path)}: CSV ë¡œë“œ ì‹¤íŒ¨. ë§ˆì§€ë§‰ ì˜¤ë¥˜: {e}")
    print(f"[ê²½ê³ ] {os.path.basename(path)}: ëª¨ë“  ì¸ì½”ë”© ì‹œë„ ì‹¤íŒ¨ â†’ utf-8 + replace ë¡œ ê°•ì œ ë¡œë“œ. ë§ˆì§€ë§‰ ì˜¤ë¥˜: {last_err}")
    return df, "utf-8(replace)"


def clean_for_analysis(df: pd.DataFrame) -> pd.DataFrame:
    """
    - ì»¬ëŸ¼ëª… ì–‘ìª½ ê³µë°± ì œê±°
    - 'Unnamed: 0' ê°™ì€ ë¤í”„ ì¸ë±ìŠ¤ ì»¬ëŸ¼ ì œê±°
    - ê³µë°±/ë¹ˆ ë¬¸ìì—´ë§Œ ìˆëŠ” ì…€ â†’ NaN(ê²°ì¸¡)ìœ¼ë¡œ í‘œì‹œ
    """
    # 1. ì»¬ëŸ¼ëª… ì •ë¦¬
    df.columns = [str(c).strip() for c in df.columns]

    # 2. Unnamed* ì»¬ëŸ¼ ì œê±°
    drop_cols = [c for c in df.columns if str(c).startswith("Unnamed")]
    if drop_cols:
        df = df.drop(columns=drop_cols)

    # 3. ê³µë°± ë˜ëŠ” íƒ­ ë“±ë§Œ ìˆëŠ” ê°’ë“¤ì„ ê²°ì¸¡ìœ¼ë¡œ ì²˜ë¦¬
    df = df.replace(r'^\s*$', pd.NA, regex=True)

    return df


def make_clean_csvs(base_path: str = BASE_PATH,
                    output_dir: str = OUTPUT_DIR,
                    output_encoding: str = "utf-8-sig"):
    """
    base_path ì•ˆ ëª¨ë“  .csv íŒŒì¼ì„:
    - ì¸ì½”ë”© ìë™ íŒë³„/ì‹œë„í•´ì„œ ë¡œë“œ
    - ê¸°ë³¸ í´ë¦° ì²˜ë¦¬ (ì»¬ëŸ¼ëª… ì •ë¦¬, Unnamed ì œê±°, ê³µë°±â†’NaN)
    - output_dir ì— `{ì›ë³¸ì´ë¦„}_clean.csv` ë¡œ ì €ì¥
    """
    # âœ… ì¶œë ¥ í´ë” ìƒì„±
    os.makedirs(output_dir, exist_ok=True)

    csv_files = glob.glob(os.path.join(base_path, "*.csv"))
    if not csv_files:
        print(f"[ì•Œë¦¼] '{base_path}' ì—ì„œ CSV íŒŒì¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return

    for path in csv_files:
        filename = os.path.basename(path)
        name, ext = os.path.splitext(filename)

        # ì´ë¯¸ _clean ìœ¼ë¡œ ëë‚˜ëŠ” íŒŒì¼ì€ ìŠ¤í‚µ
        if name.endswith("_clean"):
            continue

        try:
            df_raw, enc = read_csv_robust(path)
            df_clean = clean_for_analysis(df_raw)

            clean_name = f"{name}_clean{ext}"
            clean_path = os.path.join(output_dir, clean_name)

            df_clean.to_csv(clean_path, index=False, encoding=output_encoding)

            print(
                f"[ì™„ë£Œ] {filename} -> {clean_name} ì €ì¥ ({output_dir}) | "
                f"ì½ì€ enc={enc} | shape={df_clean.shape}"
            )
        except Exception as e:
            print(f"[ì˜¤ë¥˜] {filename} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")


# âœ… ì´ í•œ ì¤„ë§Œ ì‹¤í–‰í•˜ë©´ BASE_PATH ì•ˆ CSV â†’ OUTPUT_DIRì— *_clean.csv ìƒì„±
make_clean_csvs()



bdf = pd.read_csv("/content/drive/MyDrive/middle_project/ì¤‘ê°„í‰ê°€_ë¶„ì„íŒŒì¼_cleaná„†á…µá„€á…µá„’á…®(2025-10.3-11.2)", encoding="utf-8-sig")

import pandas as pd
import numpy as np
import requests
from datetime import datetime

# ================== ì„¤ì • ==================
SERVICE_KEY = "2403d03559e40daeeab89694df60abdabbf06848fe92122ee964798ceb14b6a9"
STN_ID = "146"  # ì „ì£¼ ASOS ì§€ì ë²ˆí˜¸
GROWTH_CSV_PATH = "/content/drive/MyDrive/middle_project/ì¤‘ê°„í‰ê°€_ë¶„ì„íŒŒì¼/á„†á…µá„€á…µá„’á…®(2025-10.3-11.2).csv"  #ì—¬ê¸°ì— íŒŒì¼ ë„£ìœ¼ë©´ í™˜ê²½ ì†”ë£¨ì…˜ í•´ì£¼ëŠ”ê±°!!! ë‹¤ë¥¸ ê±°ëŠ” ì•ˆ ë§Œì ¸ë‘ ë¨!

VPD_MIN = 0.66  # ìµœì†Ÿê°’
VPD_MAX = 0.95   # ìµœëŒ“ê°’

ASOS_DAILY_URL = "http://apis.data.go.kr/1360000/AsosDalyInfoService/getWthrDataList"


# ================== 1. ASOS ì¼ë³„ í‰ê·  ìŠµë„ + ì¼ì‚¬ ê°€ì ¸ì˜¤ê¸° ==================
def fetch_asos_daily_with_rh_rad(stn_id, start_date, end_date, service_key):
    start_dt = start_date.replace("-", "")
    end_dt = end_date.replace("-", "")

    params = {
        "serviceKey": service_key,
        "dataType": "JSON",
        "dataCd": "ASOS",
        "dateCd": "DAY",
        "startDt": start_dt,
        "endDt": end_dt,
        "stnIds": stn_id,
        "pageNo": "1",
        "numOfRows": "999",
    }

    # ìš”ì²­ íƒ€ì„ì•„ì›ƒì„ 30ì´ˆë¡œ ëŠ˜ë ¤ì¤ë‹ˆë‹¤.
    resp = requests.get(ASOS_DAILY_URL, params=params, timeout=100)
    print("[ASOS] HTTP ìƒíƒœ ì½”ë“œ:", resp.status_code)
    resp.raise_for_status()

    js = resp.json()
    items = js["response"]["body"]["items"]["item"]

    df = pd.DataFrame(items)

    # ì‚¬ìš©í•  í›„ë³´ ì»¬ëŸ¼: ë‚ ì§œ, í‰ê· ìŠµë„, ì¼ì‚¬/ì¼ì¡°
    base_cols = ["tm", "avgRhm"]
    for c in base_cols:
        if c not in df.columns:
            raise RuntimeError(f"[ASOS] ì‘ë‹µì— {c} ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. ì‹¤ì œ ì‘ë‹µ êµ¬ì¡°ë¥¼ printí•´ì„œ í™•ì¸í•´ë´ì•¼ í•©ë‹ˆë‹¤.")

    # ì¼ì‚¬ëŸ‰ ì»¬ëŸ¼ ìš°ì„ ìˆœìœ„: sumGsr(ì¼ì‚¬) > sumSsHr(ì¼ì¡°ì‹œê°„) > ì—†ìŒ
    rad_col = None
    if "sumGsr" in df.columns:
        rad_col = "sumGsr"
    elif "sumSsHr" in df.columns:
        rad_col = "sumSsHr"

    use_cols = base_cols + ([rad_col] if rad_col else [])
    df = df[use_cols].copy()

    df.rename(columns={"tm": "date", "avgRhm": "ext_humidity"}, inplace=True)

    df["date"] = pd.to_datetime(df["date"]).dt.date
    df["ext_humidity"] = pd.to_numeric(df["ext_humidity"], errors="coerce")

    if rad_col:
        df["ext_solar"] = pd.to_numeric(df[rad_col], errors="coerce")
    else:
        # ì¼ì‚¬ ì •ë³´ ì—†ìœ¼ë©´ NaN
        df["ext_solar"] = np.nan

    print("[ASOS] ì¼ìˆ˜:", len(df))
    return df


# ================== 2. ìƒìœ¡ë°ì´í„° + VPD ==================
def load_growth_and_calc_vpd(path):
    df = pd.read_csv(path, encoding="utf-8-sig")
    df.columns = [str(c).strip() for c in df.columns]

    # ì‹œê°„ ì»¬ëŸ¼
    time_candidates = ["date_time", "ì¼ì‹œ", "ìˆ˜ì§‘ì¼", "ê´€ì¸¡ì¼ì‹œ", "time", "datetime"]
    time_col = next((c for c in time_candidates if c in df.columns), None)
    if time_col is None:
        raise RuntimeError(f"[GROWTH] ì‹œê°„ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í˜„ì¬ ì»¬ëŸ¼: {list(df.columns)}")

    df[time_col] = pd.to_datetime(df[time_col])
    df["date"] = df[time_col].dt.date

    # ë‚´ë¶€ ì˜¨ë„/ìŠµë„ ì»¬ëŸ¼
    temp_candidates = ["ë‚´ë¶€-ë‚´ë¶€ì˜¨ë„", "ë‚´ë¶€ì˜¨ë„", "temperature", "ë‚´ë¶€ ì˜¨ë„"]
    hum_candidates  = ["ë‚´ë¶€-ë‚´ë¶€ìŠµë„", "ë‚´ë¶€ìŠµë„", "humidity", "ë‚´ë¶€ ìŠµë„"]

    temp_col = next((c for c in temp_candidates if c in df.columns), None)
    hum_col  = next((c for c in hum_candidates if c in df.columns), None)

    if temp_col is None or hum_col is None:
        raise RuntimeError(
            f"[GROWTH] ë‚´ë¶€ ì˜¨ë„/ìŠµë„ ì»¬ëŸ¼ ì—†ìŒ\n"
            f"ì˜¨ë„ í›„ë³´: {temp_candidates}\nìŠµë„ í›„ë³´: {hum_candidates}\n"
            f"í˜„ì¬ ì»¬ëŸ¼: {list(df.columns)}"
        )

    # VPD ê³„ì‚°
    def calc_vpd_kpa(temp_c, rh):
        es = 0.6108 * np.exp(17.27 * temp_c / (temp_c + 237.3))
        ea = es * (rh / 100.0)
        return es - ea

    df["vpd_kpa"] = calc_vpd_kpa(df[temp_col], df[hum_col])

    # ì£¼ê°„ ì—¬ë¶€
    rad_candidates = ["ë‚´ë¶€-ë‚´ë¶€ì¼ì‚¬ëŸ‰", "ë‚´ë¶€-ì¼ì‚¬", "ì¼ì‚¬ëŸ‰", "solar", "radiation"]
    rad_col = next((c for c in rad_candidates if c in df.columns), None)

    if rad_col is not None:
        df["is_daytime"] = df[rad_col] > 0
    else:
        df["is_daytime"] = df[time_col].dt.hour.between(6, 18)

    return df, time_col, temp_col, hum_col


# ================== 3. ë©”ì‹œì§€ ìƒì„± íŒŒì´í”„ë¼ì¸ ==================
def run_vpd_control():
    # 1) ë‚´ë¶€ ë°ì´í„°
    df, time_col, temp_col, hum_col = load_growth_and_calc_vpd(GROWTH_CSV_PATH)

    # 2) ASOS ì™¸ë¶€ í‰ê· ìŠµë„ + ì¼ì‚¬
    start_date = df["date"].min().strftime("%Y-%m-%d")
    end_date   = df["date"].max().strftime("%Y-%m-%d")

    df_asos = fetch_asos_daily_with_rh_rad(
        stn_id=STN_ID,
        start_date=start_date,
        end_date=end_date,
        service_key=SERVICE_KEY,
    )

    # 3) ë‚ ì§œ ê¸°ì¤€ merge
    merged = pd.merge(df, df_asos, on="date", how="left")

    # 4) ì œì–´ ë©”ì‹œì§€ ë¡œì§
    def control_msg(row):
        if not row["is_daytime"]:
            return ""

        vpd = row["vpd_kpa"]
        in_h = row[hum_col]                         # ë‚´ë¶€ ìŠµë„
        ext_h = row.get("ext_humidity", np.nan)     # ì™¸ë¶€ í‰ê·  ìƒëŒ€ìŠµë„
        ext_solar = row.get("ext_solar", np.nan)    # ì™¸ë¶€ ì¼ì‚¬ (ì¼ì‚¬ëŸ‰ or ì¼ì¡°ì‹œê°„ í•©ê³„)

        # â‘  VPD ë‚®ìŒ â†’ ë³´ê´‘ (+ ì™¸ë¶€ê°€ ë” ê±´ì¡°í•˜ë©´ í™˜ê¸°ë„ ê°™ì´ ì¶”ì²œ)
        if vpd < VPD_MIN:
            if not np.isnan(ext_h) and ext_h < in_h:
                return "ë³´ê´‘ì´ í•„ìš”í•©ë‹ˆë‹¤! / í™˜ê¸°ë¥¼ ì¶”ì²œë“œë¦½ë‹ˆë‹¤!"
            return "ë³´ê´‘ì´ í•„ìš”í•©ë‹ˆë‹¤!"

        # â‘¡ VPD ë†’ìŒ â†’ ì¼ì‚¬ëŸ‰ ê¸°ì¤€ìœ¼ë¡œ ì°¨ê´‘ vs í™˜í’ê¸°
        if vpd > VPD_MAX:
            # ì¼ì‚¬ ì •ë³´ ì—†ìœ¼ë©´: ì—´/ìŠµ ì œê±° ìœ„í•´ í™˜í’ ìœ„ì£¼
            if np.isnan(ext_solar):
                return "í™˜í’ê¸°ë¥¼ ëŒë¦¬ì„¸ìš”!"

            # ğŸ”¹ ì„ê³„ê°’ ì˜ˆì‹œ:
            # - ext_solarê°€ sumGsr(ì¼ì‚¬ëŸ‰ MJ/mÂ² ë“±) ë˜ëŠ” sumSsHr(ì¼ì¡°ì‹œê°„)ì¼ ìˆ˜ ìˆìŒ.
            # - ê°’ì´ ì–´ëŠ ì •ë„ ì´ìƒì´ë©´ 'í–‡ë¹› ì˜í–¥ í¬ë‹¤' ë³´ê³  ì°¨ê´‘ ìš°ì„ .
            #   ì—¬ê¸°ì„œëŠ” í¸ì˜ë¡œ 5 ì´ìƒì´ë©´ ì°¨ê´‘, ë¯¸ë§Œì´ë©´ í™˜í’ìœ¼ë¡œ ë¶„ê¸°.
            if ext_solar >= 5:
                return "ì°¨ê´‘ì„ í•˜ì„¸ìš”"
            else:
                return "í™˜í’ê¸°ë¥¼ ëŒë¦¬ì„¸ìš”!"

        # â‘¢ ê·¸ ì™¸: ëª©í‘œ ë²”ìœ„(0.66~0.95)ì— ê°€ê¹Œìš´ ê²½ìš° â†’ ë³„ë„ ì•¡ì…˜ ì—†ìŒ
        return ""

    merged["control_message"] = merged.apply(control_msg, axis=1)
    msg_df = merged[merged["control_message"] != ""]
    return msg_df, time_col, hum_col, merged


# ================== 4. ì¶œë ¥ ==================
if __name__ == "__main__":
    msg_df, time_col, hum_col, merged = run_vpd_control()

    if msg_df.empty:
        print("ë©”ì‹œì§€ê°€ ë‚˜ì˜¨ rowê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        # ë©”ì‹œì§€ ë°œìƒ êµ¬ê°„ ìƒìœ„ 50ê°œë§Œ ë³´ê¸°
        print(
            msg_df[[time_col, hum_col, "vpd_kpa", "ext_humidity", "ext_solar", "control_message"]]
            .head(50)
            .to_string(index=False)
        )

import pandas as pd
import numpy as np
import requests
from datetime import datetime

# ë¯¸ê¸°í›„

# ================== ì„¤ì • ==================
SERVICE_KEY = "2403d03559e40daeeab89694df60abdabbf06848fe92122ee964798ceb14b6a9"  # ì‹œì—° ë•Œ ì‹¤ì œ í‚¤ or ì˜ëª» ë„£ì–´ë„, ì‹¤íŒ¨ ì‹œ ìë™ìœ¼ë¡œ ê°€ì§œë°ì´í„° ì‚¬ìš©
STN_ID = "146"  # ì „ì£¼ ASOS ì§€ì ë²ˆí˜¸
# GROWTH_CSV_PATH = "/content/drive/MyDrive/middle_project/í”„ë¦¬ë°”(2024-11.18-11.26)_clean_correction.csv"
GROWTH_CSV_PATH = "/content/drive/MyDrive/middle_project/í”„ë¦¬ë°”(2024-11.18-11.26)_clean.csv"


VPD_MIN = 0.66  # ìµœì†Ÿê°’
VPD_MAX = 0.95  # ìµœëŒ“ê°’

ASOS_DAILY_URL = "http://apis.data.go.kr/1360000/AsosDalyInfoService/getWthrDataList"


# ================== 1. ASOS ì¼ë³„ í‰ê·  ìŠµë„ + ì¼ì‚¬ ê°€ì ¸ì˜¤ê¸° ==================
def fetch_asos_daily_with_rh_rad(stn_id, start_date, end_date, service_key):
    start_dt = start_date.replace("-", "")
    end_dt = end_date.replace("-", "")

    params = {
        "serviceKey": service_key,
        "dataType": "JSON",
        "dataCd": "ASOS",
        "dateCd": "DAY",
        "startDt": start_dt,
        "endDt": end_dt,
        "stnIds": stn_id,
        "pageNo": "1",
        "numOfRows": "999",
    }

    resp = requests.get(ASOS_DAILY_URL, params=params, timeout=20)
    print("[ASOS] HTTP ìƒíƒœ ì½”ë“œ:", resp.status_code)

    if resp.status_code != 200:
        # ì‹¤ì œ ì‹œì—°ìš©: ì‹¤íŒ¨ ì´ìœ  ì¶œë ¥í•˜ê³  ì˜ˆì™¸ ë˜ì ¸ì„œ ìƒìœ„ì—ì„œ ê°€ì§œ ë°ì´í„°ë¡œ ì „í™˜
        print("[ASOS] ì‘ë‹µ ë‚´ìš©:", resp.text[:300])
        resp.raise_for_status()

    js = resp.json()
    items = js["response"]["body"]["items"]["item"]
    df = pd.DataFrame(items)

    # ì‚¬ìš©í•  í›„ë³´ ì»¬ëŸ¼: ë‚ ì§œ, í‰ê· ìŠµë„, ì¼ì‚¬/ì¼ì¡°
    base_cols = ["tm", "avgRhm"]
    for c in base_cols:
        if c not in df.columns:
            raise RuntimeError(f"[ASOS] ì‘ë‹µì— {c} ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. ì‹¤ì œ ì‘ë‹µ êµ¬ì¡°ë¥¼ í™•ì¸í•˜ì„¸ìš”.")

    # ì¼ì‚¬ëŸ‰ ì»¬ëŸ¼ ìš°ì„ ìˆœìœ„
    rad_col = None
    if "sumGsr" in df.columns:
        rad_col = "sumGsr"
    elif "sumSsHr" in df.columns:
        rad_col = "sumSsHr"

    use_cols = base_cols + ([rad_col] if rad_col else [])
    df = df[use_cols].copy()

    df.rename(columns={"tm": "date", "avgRhm": "ext_humidity"}, inplace=True)
    df["date"] = pd.to_datetime(df["date"]).dt.date
    df["ext_humidity"] = pd.to_numeric(df["ext_humidity"], errors="coerce")

    if rad_col:
        df["ext_solar"] = pd.to_numeric(df[rad_col], errors="coerce")
    else:
        df["ext_solar"] = np.nan

    print("[ASOS] ì¼ìˆ˜:", len(df))
    return df


# ============= 1-1. API ì‹¤íŒ¨ ì‹œ ì‚¬ìš©í•  ê°€ì§œ ì™¸ë¶€ ë°ì´í„° ìƒì„± =============
def make_fake_asos_from_growth(df_growth, hum_col):
    """
    ASOS API ì‹¤íŒ¨ ì‹œ, ë‚´ë¶€ ë°ì´í„° ê¸°ê°„ì— ë§ì¶°
    'ê·¸ëŸ´ë“¯í•œ' ì™¸ë¶€ ìŠµë„/ì¼ì‚¬ ë°ì´í„°ë¥¼ ë§Œë“œëŠ” í•¨ìˆ˜ (ì‹œì—°ìš©).
    """
    print("[FAKE ASOS] ì‹¤ì œ API ì‹¤íŒ¨ â†’ ì‹œì—°ìš© ê°€ì§œ ì™¸ë¶€ ë°ì´í„° ìƒì„±í•©ë‹ˆë‹¤.")

    dates = sorted(df_growth["date"].unique())
    np.random.seed(42)  # ì‹œì—° ë•Œë§ˆë‹¤ ê°™ì€ ê°’ ë‚˜ì˜¤ë„ë¡ ê³ ì •

    fake_rows = []
    for d in dates:
        inside_mean_h = df_growth.loc[df_growth["date"] == d, hum_col].mean()

        # ì™¸ë¶€ ìŠµë„: ë‚´ë¶€ë³´ë‹¤ ëŒ€ì²´ë¡œ ì•½ê°„ ë‚®ê±°ë‚˜ ë¹„ìŠ·í•˜ê²Œ (30~95% ì‚¬ì´ í´ë¦¬í•‘)
        ext_h = inside_mean_h - np.random.uniform(-5, 15)
        ext_h = float(np.clip(ext_h, 30, 95))

        # ì™¸ë¶€ ì¼ì‚¬: ëŒ€ì¶© ë§‘ì€ë‚ /íë¦°ë‚  ì„ì¸ ëŠë‚Œ (0~12 ì‚¬ì´)
        ext_solar = float(np.random.choice([0, 2, 4, 6, 8, 10, 12], p=[0.1,0.1,0.15,0.2,0.2,0.15,0.1]))

        fake_rows.append({
            "date": d,
            "ext_humidity": ext_h,
            "ext_solar": ext_solar,
        })

    fake_df = pd.DataFrame(fake_rows)
    print("[FAKE ASOS] ìƒì„±ëœ ì¼ìˆ˜:", len(fake_df))
    return fake_df


# ================== 2. ìƒìœ¡ë°ì´í„° + VPD ==================
def load_growth_and_calc_vpd(path):
    df = pd.read_csv(path, encoding="utf-8-sig")
    df.columns = [str(c).strip() for c in df.columns]

    # ì‹œê°„ ì»¬ëŸ¼
    time_candidates = ["date_time", "ì¼ì‹œ", "ìˆ˜ì§‘ì¼", "ê´€ì¸¡ì¼ì‹œ", "time", "datetime"]
    time_col = next((c for c in time_candidates if c in df.columns), None)
    if time_col is None:
        raise RuntimeError(f"[GROWTH] ì‹œê°„ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í˜„ì¬ ì»¬ëŸ¼: {list(df.columns)}")

    df[time_col] = pd.to_datetime(df[time_col])
    df["date"] = df[time_col].dt.date

    # ë‚´ë¶€ ì˜¨ë„/ìŠµë„ ì»¬ëŸ¼
    temp_candidates = ["ë‚´ë¶€-ë‚´ë¶€ì˜¨ë„", "ë‚´ë¶€ì˜¨ë„", "temperature", "ë‚´ë¶€ ì˜¨ë„"]
    hum_candidates  = ["ë‚´ë¶€-ë‚´ë¶€ìŠµë„", "ë‚´ë¶€ìŠµë„", "humidity", "ë‚´ë¶€ ìŠµë„"]

    temp_col = next((c for c in temp_candidates if c in df.columns), None)
    hum_col  = next((c for c in hum_candidates if c in df.columns), None)

    if temp_col is None or hum_col is None:
        raise RuntimeError(
            f"[GROWTH] ë‚´ë¶€ ì˜¨ë„/ìŠµë„ ì»¬ëŸ¼ ì—†ìŒ\n"
            f"ì˜¨ë„ í›„ë³´: {temp_candidates}\nìŠµë„ í›„ë³´: {hum_candidates}\n"
            f"í˜„ì¬ ì»¬ëŸ¼: {list(df.columns)}"
        )

    # VPD ê³„ì‚°
    def calc_vpd_kpa(temp_c, rh):
        es = 0.6108 * np.exp(17.27 * temp_c / (temp_c + 237.3))
        ea = es * (rh / 100.0)
        return es - ea

    df["vpd_kpa"] = calc_vpd_kpa(df[temp_col], df[hum_col])

    # ì£¼ê°„ ì—¬ë¶€
    rad_candidates = ["ë‚´ë¶€-ë‚´ë¶€ì¼ì‚¬ëŸ‰", "ë‚´ë¶€-ì¼ì‚¬", "ì¼ì‚¬ëŸ‰", "solar", "radiation"]
    rad_col = next((c for c in rad_candidates if c in df.columns), None)

    if rad_col is not None:
        df["is_daytime"] = df[rad_col] > 0
    else:
        df["is_daytime"] = df[time_col].dt.hour.between(6, 18)

    return df, time_col, temp_col, hum_col


# ================== 3. ë©”ì‹œì§€ ìƒì„± íŒŒì´í”„ë¼ì¸ ==================
def run_vpd_control():
    # 1) ë‚´ë¶€ ë°ì´í„°
    df, time_col, temp_col, hum_col = load_growth_and_calc_vpd(GROWTH_CSV_PATH)

    # 2) ASOS ì™¸ë¶€ í‰ê· ìŠµë„ + ì¼ì‚¬ (ì‹¤íŒ¨ ì‹œ ê°€ì§œë°ì´í„° ì‚¬ìš©)
    start_date = df["date"].min().strftime("%Y-%m-%d")
    end_date   = df["date"].max().strftime("%Y-%m-%d")

    try:
        df_asos = fetch_asos_daily_with_rh_rad(
            stn_id=STN_ID,
            start_date=start_date,
            end_date=end_date,
            service_key=SERVICE_KEY,
        )
        print("[INFO] ì‹¤ì œ ASOS ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    except Exception as e:
        print("[WARN] ASOS API í˜¸ì¶œ ì‹¤íŒ¨:", e)
        df_asos = make_fake_asos_from_growth(df, hum_col)
        print("[INFO] ì‹œì—°ìš© ê°€ì§œ ASOS ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")

    # 3) ë‚ ì§œ ê¸°ì¤€ merge
    merged = pd.merge(df, df_asos, on="date", how="left")

    # 4) ì œì–´ ë©”ì‹œì§€ ë¡œì§
    def control_msg(row):
        if not row["is_daytime"]:
            return ""

        vpd = row["vpd_kpa"]
        in_h = row[hum_col]                         # ë‚´ë¶€ ìŠµë„
        ext_h = row.get("ext_humidity", np.nan)     # ì™¸ë¶€ í‰ê·  ìƒëŒ€ìŠµë„
        ext_solar = row.get("ext_solar", np.nan)    # ì™¸ë¶€ ì¼ì‚¬ (ì¼ì‚¬ëŸ‰ or ì¼ì¡°ì‹œê°„ í•©ê³„)

        # â‘  VPD ë‚®ìŒ â†’ ë³´ê´‘ (+ ì™¸ë¶€ê°€ ë” ê±´ì¡°í•˜ë©´ í™˜ê¸°ë„ ê°™ì´ ì¶”ì²œ)
        if vpd < VPD_MIN:
            if not np.isnan(ext_h) and ext_h < in_h:
                return "ë³´ê´‘ì´ í•„ìš”í•©ë‹ˆë‹¤! / í™˜ê¸°ë¥¼ ì¶”ì²œë“œë¦½ë‹ˆë‹¤!"
            return "ë³´ê´‘ì´ í•„ìš”í•©ë‹ˆë‹¤!"

        # â‘¡ VPD ë†’ìŒ â†’ ì¼ì‚¬ëŸ‰ ê¸°ì¤€ìœ¼ë¡œ ì°¨ê´‘ vs í™˜í’ê¸°
        if vpd > VPD_MAX:
            if np.isnan(ext_solar):
                return "í™˜í’ê¸°ë¥¼ ëŒë¦¬ì„¸ìš”!"
            if ext_solar >= 5:
                return "ì°¨ê´‘ì„ í•˜ì„¸ìš”"
            else:
                return "í™˜í’ê¸°ë¥¼ ëŒë¦¬ì„¸ìš”!"

        # â‘¢ ê·¸ ì™¸: ëª©í‘œ ë²”ìœ„(0.66~0.95) ê·¼ì²˜ â†’ ì•¡ì…˜ ì—†ìŒ
        return ""

    merged["control_message"] = merged.apply(control_msg, axis=1)
    msg_df = merged[merged["control_message"] != ""]
    return msg_df, time_col, hum_col, merged


# ================== 4. ì¶œë ¥ ==================
if __name__ == "__main__":
    msg_df, time_col, hum_col, merged = run_vpd_control()


    total_count = len(msg_df)
    print(f"\n=== ì†”ë£¨ì…˜ ì´ ê°œìˆ˜: {total_count}ê°œ ===")

    if msg_df.empty:
        print("ë©”ì‹œì§€ê°€ ë‚˜ì˜¨ rowê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        print("=== ì†”ë£¨ì…˜ ë°œìƒ ì‹œì  (ìƒìœ„ 50ê°œ) ===")
        print(
            msg_df[[time_col, hum_col, "vpd_kpa", "ext_humidity", "ext_solar", "control_message"]]
            .head()    #ìš”ê¸°!
            .to_string(index=False)
        )

        # ê°€ì¥ ìµœì‹  ì†”ë£¨ì…˜ 1ê°œ
        latest = msg_df.sort_values(time_col).iloc[-1]
        print("\n=== ê°€ì¥ ìµœì‹  ì†”ë£¨ì…˜ 1ê°œ ===")
        print(
            f"ì‹œê°„: {latest[time_col]} | "
            f"ë‚´ë¶€ìŠµë„: {latest[hum_col]:.1f}% | "
            f"ì™¸ë¶€ìŠµë„: {latest['ext_humidity'] if not np.isnan(latest['ext_humidity']) else 'NaN'} | "
            f"ì™¸ë¶€ì¼ì‚¬: {latest['ext_solar'] if not np.isnan(latest['ext_solar']) else 'NaN'} | "
            f"VPD: {latest['vpd_kpa']:.3f} kPa | "
            f"ì¶”ì²œ: {latest['control_message']}"
        )

if __name__ == "__main__":
    msg_df, time_col, hum_col, merged = run_vpd_control()

    # ë¶„ì„ ê¸°ê°„
    start_date = merged["date"].min()
    end_date = merged["date"].max()

    if msg_df.empty:
        print("=== VPD ê¸°ë°˜ ì˜¨ì‹¤ ì œì–´ ìš”ì•½ ë¦¬í¬íŠ¸ ===")
        print(f"- ë¶„ì„ ê¸°ê°„ : {start_date} ~ {end_date}")
        print("- ì§€ì •í•œ ì¡°ê±´(VPD, ì£¼ê°„)ì— í•´ë‹¹í•˜ëŠ” ì œì–´ ê¶Œì¥ ìƒí™©ì´ ë°œìƒí•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    else:
        # âœ… 1. ì´ ê°œìˆ˜
        total = len(msg_df)

        # âœ… 2. ë©”ì‹œì§€ ìœ í˜• ì •ë¦¬
        type_map = {
            "ë³´ê´‘ì´ í•„ìš”í•©ë‹ˆë‹¤! / í™˜ê¸°ë¥¼ ì¶”ì²œë“œë¦½ë‹ˆë‹¤!": "ë³´ê´‘ + í™˜ê¸°",
            "ë³´ê´‘ì´ í•„ìš”í•©ë‹ˆë‹¤!": "ë³´ê´‘",
            "ì°¨ê´‘ì„ í•˜ì„¸ìš”": "ì°¨ê´‘",
            "í™˜í’ê¸°ë¥¼ ëŒë¦¬ì„¸ìš”!": "í™˜í’ê¸°"
        }
        msg_df["action_type"] = msg_df["control_message"].map(type_map).fillna("ê¸°íƒ€")

        type_counts = msg_df["action_type"].value_counts()

        # âœ… 3. ë‚ ì§œë³„ ë°œìƒ íšŸìˆ˜
        daily_counts = (
            msg_df.groupby("date")["control_message"]
            .count()
            .sort_values(ascending=False)
        )

        # âœ… 4. ê°€ì¥ ìµœì‹  ì œì–´ ì‚¬ë¡€
        latest = msg_df.sort_values(time_col).iloc[-1]

        # âœ… ë³´ê¸° ì¢‹ì€ ë¦¬í¬íŠ¸ ì¶œë ¥
        print("=== VPD ê¸°ë°˜ ì˜¨ì‹¤ ì œì–´ ìš”ì•½ ë¦¬í¬íŠ¸ ===")
        print(f"- ë¶„ì„ ê¸°ê°„            : {start_date} ~ {end_date}")
        print(f"- ì´ ì œì–´ ê¶Œì¥ íšŸìˆ˜    : {total}íšŒ")

        print("\n[ê¶Œì¥ ì•¡ì…˜ ìœ í˜•ë³„ ë°œìƒ ë¹ˆë„]")
        for t, c in type_counts.items():
            print(f"  Â· {t:<8}: {c}íšŒ")

        print("\n[ë‚ ì§œë³„ ì œì–´ ë©”ì‹œì§€ ë°œìƒ TOP 7]")
        print(daily_counts.head(7).to_string(header=False))

        print("\n[ëŒ€í‘œ ì œì–´ ì‚¬ë¡€ (ê°€ì¥ ìµœê·¼)]")
        print(
            f"  Â· ì‹œê°„       : {latest[time_col]}\n"
            f"  Â· ë‚´ë¶€ìŠµë„   : {latest[hum_col]:.1f}%\n"
            f"  Â· ì™¸ë¶€ìŠµë„   : {latest['ext_humidity'] if not np.isnan(latest['ext_humidity']) else 'NaN'}%\n"
            f"  Â· ì™¸ë¶€ì¼ì‚¬   : {latest['ext_solar'] if not np.isnan(latest['ext_solar']) else 'NaN'}\n"
            f"  Â· VPD        : {latest['vpd_kpa']:.3f} kPa\n"
            f"  Â· ê¶Œì¥ ì•¡ì…˜  : {latest['control_message']}"
        )

        print("\n[ìƒì„¸ ë¡œê·¸ ìƒìœ„ 30ê±´]")
        print(
            msg_df[[time_col, hum_col, "vpd_kpa", "ext_humidity", "ext_solar", "control_message"]]
            .head(30)
            .to_string(index=False)
        )